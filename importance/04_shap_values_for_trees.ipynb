{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09db8e4b",
   "metadata": {},
   "source": [
    "# üßÆ 04 ‚Äî SHAP Values for Tree Models\n",
    "In this notebook, we‚Äôll learn how to:\n",
    "- Use SHAP (SHapley Additive exPlanations) for interpretability\n",
    "- Understand **global** and **local** feature importance\n",
    "- Apply SHAP with tree-based models like Random Forest and XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc5ae3b",
   "metadata": {},
   "source": [
    "## üì• 1. Load Dataset & Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a526c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec4bf16",
   "metadata": {},
   "source": [
    "## üìä 2. Compute SHAP Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3072665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Use TreeExplainer\n",
    "explainer = shap.Explainer(model, X_train)\n",
    "shap_values = explainer(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892df2cf",
   "metadata": {},
   "source": [
    "## üìà 3. SHAP Summary Plot (Global Importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa0c386",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values, max_display=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e28b05",
   "metadata": {},
   "source": [
    "## üîç 4. SHAP Force Plot (Local Explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67700cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if using Jupyter notebook interface\n",
    "# shap.initjs()\n",
    "\n",
    "# Visualize explanation for one instance\n",
    "shap.plots.force(shap_values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e313165",
   "metadata": {},
   "source": [
    "## üß† 5. Why Use SHAP?\n",
    "- SHAP values offer **consistent** and **local** feature attributions\n",
    "- Help explain **individual predictions**\n",
    "- Visualizations help build trust in models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cd5a36",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary\n",
    "- SHAP values explain predictions by feature contribution\n",
    "- Summary plots show global importance\n",
    "- Force plots reveal local explanations for individual predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fc78d3",
   "metadata": {},
   "source": [
    "üéâ You've completed the **Model Interpretability** module!\n",
    "You can now confidently explain models using SHAP, PDP, permutation, and feature importance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
