{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "80ca266e",
      "metadata": {
        "id": "80ca266e"
      },
      "source": [
        "# 03 — Classification Metrics: Accuracy, Precision, Recall, F1\n",
        "In this notebook, we'll learn how to:\n",
        "- Evaluate classification models beyond just accuracy\n",
        "- Use precision, recall, and F1-score\n",
        "- Interpret these metrics based on the confusion matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c39edc3",
      "metadata": {
        "id": "3c39edc3"
      },
      "source": [
        "## 1. Load Dataset & Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dd54076",
      "metadata": {
        "id": "9dd54076"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Split into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model = LogisticRegression(max_iter=500)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6401e99c",
      "metadata": {
        "id": "6401e99c"
      },
      "source": [
        "## 2. Accuracy, Precision, Recall, F1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ba7b667",
      "metadata": {
        "id": "4ba7b667"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "prec = precision_score(y_test, y_pred)\n",
        "rec = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {acc:.3f}\")\n",
        "print(f\"Precision: {prec:.3f}\")\n",
        "print(f\"Recall: {rec:.3f}\")\n",
        "print(f\"F1 Score: {f1:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4920af34",
      "metadata": {
        "id": "4920af34"
      },
      "source": [
        "## 3. Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64a4d342",
      "metadata": {
        "id": "64a4d342"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=data.target_names)\n",
        "disp.plot(cmap=\"Blues\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56f040ea",
      "metadata": {
        "id": "56f040ea"
      },
      "source": [
        "## 4. What Do These Metrics Mean?\n",
        "- **Accuracy**: Overall correctness\n",
        "- **Precision**: Correctness among positive predictions\n",
        "- **Recall**: Coverage of actual positives\n",
        "- **F1 Score**: Harmonic mean of precision and recall\n",
        "\n",
        "Useful when:\n",
        "- Precision matters more (e.g. spam detection)\n",
        "- Recall matters more (e.g. cancer detection)\n",
        "- Or both (F1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d3b8e5d",
      "metadata": {
        "id": "1d3b8e5d"
      },
      "source": [
        "## Summary\n",
        "- Accuracy alone can be misleading in imbalanced datasets\n",
        "- Precision, recall, and F1 provide deeper insight\n",
        "- Confusion matrix helps interpret errors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c78a1ee6",
      "metadata": {
        "id": "c78a1ee6"
      },
      "source": [
        "## What’s Next?\n",
        "In the next notebook:\n",
        "**`04_regression_metrics.ipynb`** — we’ll evaluate regression models using MAE, MSE, and R²."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}