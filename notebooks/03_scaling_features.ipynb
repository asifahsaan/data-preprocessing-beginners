{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNr7gIW19iBsnPJVFC2AIF6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üìí Notebook: 03_scaling_features.ipynb\n",
        "üìå Sections:\n",
        "Title & Introduction\n",
        "\n",
        "\n",
        "\n",
        "1.   Why Scaling is Important\n",
        "2.   Visualizing Raw (Unscaled) Features\n",
        "3. Standard Scaling (StandardScaler)\n",
        "4. Min-Max Scaling (MinMaxScaler)\n",
        "5. Compare Transformed Values\n",
        "6. When to Use Which Scaler\n",
        "7. Summary / What‚Äôs Next\n",
        "\n"
      ],
      "metadata": {
        "id": "gfiKOa6WkedN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Title & Introduction (Markdown Cell)\n",
        "### üìè 03 ‚Äî Scaling and Normalizing Features\n",
        "\n",
        "In this notebook, we'll learn:\n",
        "\n",
        "- Why feature scaling is important\n",
        "- How to apply **StandardScaler** and **MinMaxScaler**\n",
        "- The difference between standardization and normalization\n",
        "- How to compare original vs. scaled feature distributions\n",
        "\n",
        "### 2. Why Scaling is Important (Markdown Cell)\n",
        "## ü§î Why Do We Need Scaling?\n",
        "\n",
        "Many machine learning models (like k-NN, SVM, and neural nets) are sensitive to the **scale** of input features.\n",
        "\n",
        "If one column has values ranging from 1‚Äì1000, and another from 0‚Äì1, the model may give too much importance to the large-scale feature.\n",
        "\n",
        "üëâ We fix this using **scaling**.\n"
      ],
      "metadata": {
        "id": "xQnsjLSFlAWg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Load and Visualize Raw Data"
      ],
      "metadata": {
        "id": "FW5bT6QIlP3v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNNe7NrHkd3K"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"../data/sample_data.csv\")\n",
        "\n",
        "# Choose a few numeric columns\n",
        "numeric_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "\n",
        "# Quick look\n",
        "df[numeric_cols].head()\n",
        "\n",
        "# Histogram of raw data\n",
        "df[numeric_cols].hist(figsize=(10, 6), bins=20)\n",
        "plt.suptitle(\"Original Feature Distributions\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Apply StandardScaler"
      ],
      "metadata": {
        "id": "pgDT5WYNlZGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler_std = StandardScaler()\n",
        "scaled_std = scaler_std.fit_transform(df[numeric_cols])\n",
        "\n",
        "# Convert to DataFrame for inspection\n",
        "df_scaled_std = pd.DataFrame(scaled_std, columns=[col + \"_std\" for col in numeric_cols])\n",
        "df_scaled_std.head()"
      ],
      "metadata": {
        "id": "Qioc-wmDldc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xuekeSQ7lfYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler_mm = MinMaxScaler()\n",
        "scaled_mm = scaler_mm.fit_transform(df[numeric_cols])\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_scaled_mm = pd.DataFrame(scaled_mm, columns=[col + \"_mm\" for col in numeric_cols])\n",
        "df_scaled_mm.head()"
      ],
      "metadata": {
        "id": "ukzCn9LGlff4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kCffMI1HlhGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize distributions after scaling\n",
        "df_scaled_std.hist(figsize=(10, 6), bins=20)\n",
        "plt.suptitle(\"Standard Scaled Features (mean=0, std=1)\")\n",
        "plt.show()\n",
        "\n",
        "df_scaled_mm.hist(figsize=(10, 6), bins=20)\n",
        "plt.suptitle(\"Min-Max Scaled Features (range 0 to 1)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "j3dktaAxlhNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. When to Use Which Scaler (Markdown Cell)\n",
        "### ü§ì StandardScaler vs. MinMaxScaler ‚Äî When to Use\n",
        "\n",
        "**StandardScaler**  \n",
        "- Centers data (mean = 0, std = 1)  \n",
        "- Use when features are normally distributed  \n",
        "- Good for: Linear regression, logistic regression, SVM\n",
        "\n",
        "**MinMaxScaler**  \n",
        "- Scales to a 0‚Äì1 range  \n",
        "- Use when you want bounded inputs  \n",
        "- Good for: Neural networks, clustering, distance-based algorithms\n"
      ],
      "metadata": {
        "id": "BiYyjdUGljXL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Summary / What‚Äôs Next (Markdown Cell)\n",
        "### ‚úÖ Summary\n",
        "\n",
        "In this notebook, we:\n",
        "\n",
        "- Explored why scaling is necessary\n",
        "- Applied **StandardScaler** (standardization)\n",
        "- Applied **MinMaxScaler** (normalization)\n",
        "- Compared distributions before and after scaling\n",
        "\n",
        "‚û°Ô∏è **Next Up**: `04_encoding_categorical_variables.ipynb`  \n",
        "We'll learn how to encode categorical columns using **OneHotEncoder** and **OrdinalEncoder**.\n"
      ],
      "metadata": {
        "id": "Lnp0TIZJlo1w"
      }
    }
  ]
}