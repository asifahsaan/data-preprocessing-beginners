{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a44ed3e",
   "metadata": {},
   "source": [
    "# ðŸ“Š 01 â€” Classification Metrics (Accuracy, Precision, Recall)\n",
    "In this notebook, weâ€™ll cover the most common classification metrics to evaluate binary classifiers:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 Score\n",
    "- Confusion Matrix\n",
    "\n",
    "Weâ€™ll also visualize the metrics using a confusion matrix heatmap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3959c33",
   "metadata": {},
   "source": [
    "## ðŸ“¥ 1. Load Dataset and Train a Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee1a882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load data\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0edbc1a",
   "metadata": {},
   "source": [
    "## âœ… 2. Accuracy, Precision, Recall, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e97963a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1269ea2",
   "metadata": {},
   "source": [
    "## ðŸ§® 3. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42e6af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6639486",
   "metadata": {},
   "source": [
    "## ðŸ”Ž 4. When to Use Which Metric?\n",
    "- **Accuracy**: Use when classes are balanced\n",
    "- **Precision**: Use when false positives are costly\n",
    "- **Recall**: Use when false negatives are costly (e.g. medical screening)\n",
    "- **F1 Score**: Harmonic mean of precision & recall (best of both worlds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f96e775",
   "metadata": {},
   "source": [
    "## âœ… Summary\n",
    "- Accuracy, precision, recall, and F1 are key classification metrics\n",
    "- Confusion matrix helps visualize classification results\n",
    "- Choose metric based on use-case (false positives vs false negatives)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
